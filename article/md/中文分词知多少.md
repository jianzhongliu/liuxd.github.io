# 中文分词知多少 #

## 一、概述 ##

分词，顾名思义，就是把一段文字划分成多个词的动作。它是自然语言处理的重要分支。分词的终极目标，是在尽可能短的时间里将一段文字分解成尽可能接近用户本意的多个词。

### 1.流派 ###

自然语言处理的发展史上曾经有两大流派：语法派，统计派。

语法派，是期望计算机能像人一样用语法规则来分析语言，经过一代科学家走了20多年弯路之后，人们发现这条路不切实际，没有再继续探索的价值。

统计派，是用数学方法通过大量语料来训练计算机，从而让计算机知道哪些是词，哪些不是，并且能联系上下文进行分词选择，这种思路发展至今已经比较成熟。目前的分词方案都是基于这个思路实现的。

### 2.优化 ###

改善分词质量主要从两个方面着手：算法，词库。

（一）合理的算法可以提高分词性能、提高歧义处理能力。

（二）强大的词库可以提高词汇的识别率。与时俱进的词库对分词有决定性的作用。词库需要一个完整的训练机制来维护，使得词库越来越强大。

### 3.难点 ###

__(1)覆盖歧义__

例子：开心辞典是个好节目，“开心”是一个词，“开心辞典”也是一个词，究竟该按哪个词来分呢？“开心”之后可下刀否？

__(2)交叉歧义__

例子：北京大学生的素质普遍比较高，是该“北京大学/生”？还是该“北京/大学生”？这种左拥右抱的货色该怎么处置？

__(3)语义歧义__

例子：“白天鹅在水中游”，这句话是有歧义的，它可能指的是“白天有一只鹅在水中游”，也可能指的是“有一只白天鹅在水中游”。别说是计算机，就是活人来解读也无法确定其真实含义。

__(4)新词识别__

要识别词势必离不开词库，可是“时代在发展、社会在进步”，大量新鲜事物必然带来大量新词汇，如果词库不能及时更新，那么分词的时候就不认识这些词了。比如五年前的分词系统的词库肯定没有“坑爹”这个词。

__(5)错别字__

用户的输入是不可靠的，不是每个人都能把心理想要的东西准确表达出来，但是分词系统不能因此而抛弃他们。比如：黄粱一梦，如果用户输入“黄梁一梦”呢？难道把这四个字分成四个单字吗？那实在是太弱了。


## 二、分词算法之条件随机场（CRF） ##

CRF——Conditional Random Field.

条件随机场，是目前分词系统中广泛使用的算法。要了解条件随机场就要从马尔可夫链开始讲起。

### 1.马尔可夫链 ###

如果随机变量m在时间t所处的状态用S<sub>t</sub>来表示，S<sub>1</sub>，S<sub>2</sub>... ...S<sub>t</sub>表示随机变量m在时间1~t之间的状态。假设S<sub>t</sub>只和S<sub>t-1</sub>有关，和其他状态无关，这一假设称为马尔可夫假设，符合这一假设的随机过程称为马尔可夫过程，也叫马尔可夫链。
    
对于马尔可夫链，如果已知状态序列S<sub>1</sub>，S<sub>2</sub>... ...S<sub>t</sub>，则很容易指定随机变量m的变化情况，包括变化成的值和变成该值的次数，从而算出变成各值的概率。

### 2.隐含马尔可夫模型 ###
    
隐含马尔可夫模型是马尔可夫链的一个扩展：随机变量m在任一时刻t的状态S<sub>t</sub>是不可见的。但是会输出一个符号O<sub>t</sub>，这个O<sub>t</sub>和且仅和S<sub>t</sub>相关。这就是隐含马尔可夫模型。

### 3.条件随机场模型 ###
	
在隐含马尔可夫模型里，观察值O<sub>t</sub>只和号S<sub>t</sub>有关。如果扩大O<sub>t</sub>的考虑因素范围，同时和S<sub>t-1</sub>、S<sub>t</sub>、S<sub>t+1</sub>有关，这样的模型就是条件随机场模型。

## 三、分词质量评估（PRF） ##

分词质量评估主要考虑PRF三个指标：准确率（Ｐ）、召回率（Ｒ）、综合指标Ｆ值（Ｆ）。

__准确率（Ｐ）__ = 准确切分的词语数 / 切分出的所有词语数

__召回率（Ｒ）__ = 准确切分的词语数 / 应该切分的词语数

__综合指标Ｆ值（Ｆ）__ = （β的平方＋１）ＰＲ/（β的平方*Ｐ＋Ｒ）

其中，β决定对Ｐ侧重还是对Ｒ侧重，通常设定为１、２或１／２。β取值为１，即对二者一样重视。

## 四、斯坦福分词系统 ##

### 1.介绍 ###

[官方网站](http://nlp.stanford.edu/software/segmenter.shtml)

这是一个基于统计的分词系统。要想充分发挥作用需要有强大的词库来配合。词库的来源有两种：

__(1).别人发布的词库__

目前已知的词库有两个，一个是斯坦福大学的PKU词库。另一个是北京大学的CTB词库。

__(2).自己训练词库__

自己动手，丰衣足食。自己训练的词库有更强的针对性，但是开发成本更高。训练词库并不是一件轻松容易的事。

### 2.现状 ###

目前安居客对斯坦福分词系统的使用情况如下：

__(1).部署__

* 集成了jetty。
* 将对斯坦福分词的调用写到Servlet内。
* 使用的包：edu.stanford.nlp.ie.crf。
* 调用的类：CRFClassifier。
* 使用的词库：CTB。
* 机器：app10-048上。
* 开启了三个服务进程。
* 通过nginx实现负载均衡。

关键配置如下：

    location /chseg {
        proxy_pass http://chseg_service/seg/ctbseg;
    }
    
    upstream chseg_service {
        server 127.0.0.1:8990;
        server 127.0.0.1:8991;
        server 127.0.0.1:8992;
    }

__(2).配置__

v2代码中的相关配置：

    /home/www/v2/haozu/config/search.php
    $config['segment_url'] = "http://$host:8080/chseg";
    
__(3).调用__

    http://$host:$port/seg/ctbseg?text=$text

$host：服务所在主机。

$port：服务监听的端口。

$text：待分词文本。

### 3.调试 ###

本地建立斯坦福分词服务是比较简单的。

>下载代码：git clone git@git.corp.anjuke.com:corp/chinese-segment.git

>切换目录：cd chinese-segment

>启动服务：java -jar ./start.jar。

>测试服务：curl -v http://127.0.0.1:8999/seg/ctbseg?text=一个非常不错的东西

顺利的话就会看到这样的结果：

    * About to connect() to 127.0.0.1 port 8999 (#0)
    *   Trying 127.0.0.1... connected
    * Connected to 127.0.0.1 (127.0.0.1) port 8999 (#0)
    > GET /seg/ctbseg?text=一个非常不错的东西 HTTP/1.1
    > User-Agent: curl/7.19.7 (x86_64-pc-linux-gnu) libcurl/7.19.7         OpenSSL/0.9.8k zlib/1.2.3.3 libidn/1.15
    > Host: 127.0.0.1:8999
    > Accept: */*
    > 
    < HTTP/1.1 200 OK
    < Content-Length: 33
    < 
    * Connection #0 to host 127.0.0.1 left intact
    * Closing connection #0
    一 个 非常 不错 的 东西
